{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "def compare_pair(lst):\n",
    "    result = 0\n",
    "    # compare left to right, if left < right return 0...\n",
    "    if lst[0] < lst[1]:\n",
    "        result = 0\n",
    "    elif lst[0] == lst[1]:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 2\n",
    "    return result\n",
    "\n",
    "def label_to_pair_compare(lst):\n",
    "    return torch.Tensor([compare_pair(each) for each in lst]).long()\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    # batch size * 2 samples\n",
    "    text_batch = [each_item['text'] for each_item in batch]\n",
    "\n",
    "    # reshape to [batch size * 2]\n",
    "    text_batch = list(chunks(text_batch, args.slate_num))\n",
    "    \n",
    "    label_batch = torch.stack([each_item['label'] for each_item in batch]).float()\n",
    "    # reshape to [batch size * 2]\n",
    "    label_batch = label_batch.reshape(-1,args.slate_num) \n",
    "\n",
    "    return text_batch, label_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LSTModel(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.linear(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(pl.LightningModule):\n",
    "    def __init__(self, input_size, , output_size, class_num, hidden_size, projection_size):\n",
    "        super(LSTM_model, self).__init__()\n",
    "\n",
    "        bsz = 1\n",
    "        self.direction = 2\n",
    "        self.input_size = input_size\n",
    "        self.slate_num = slate_num\n",
    "        self.output_size = output_size\n",
    "        self.class_num = class_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.projection_size = projection_size\n",
    "        self.batch_size = bsz\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_size, \n",
    "            self.hidden_size, \n",
    "            num_layers=1, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Linear(\n",
    "            self.hidden_size * self.direction, \n",
    "            self.projection_size\n",
    "        )\n",
    "\n",
    "        self.ranker = nn.Linear(\n",
    "            self.projection_size * self.slate_num, \n",
    "            self.output_size\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(\n",
    "            self.hidden_size * self.direction, \n",
    "            self.class_num\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, x, batch_size = None):\n",
    "        #print(x)\n",
    "\n",
    "        lens = [len(sq) for sq in x]\n",
    "        x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "        x = pack_padded_sequence(x, lens, batch_first=True, enforce_sorted=False)\n",
    "        #print(x.data)\n",
    "\n",
    "        if batch_size is None:\n",
    "            # Initial hidden state of the LSTM (num_layers * num_directions, batch, hidden_size)\n",
    "            h_0 = torch.zeros(\n",
    "                1 * self.direction, \n",
    "                self.batch_size, \n",
    "                self.hidden_size\n",
    "            ).requires_grad_()#.to(device=to_device)\n",
    "            \n",
    "            # Initial cell state of the LSTM\n",
    "            c_0 = torch.zeros(\n",
    "                1 * self.direction, \n",
    "                self.batch_size, \n",
    "                self.hidden_size\n",
    "            ).requires_grad_()#.to(device=to_device)\n",
    "\n",
    "        else:\n",
    "            h_0 = torch.zeros(\n",
    "                1 * self.direction, \n",
    "                batch_size, \n",
    "                self.hidden_size\n",
    "            ).requires_grad_()#.to(device=to_device)\n",
    "            \n",
    "            c_0 = torch.zeros(\n",
    "                1 * self.direction, \n",
    "                batch_size, \n",
    "                self.hidden_size\n",
    "            ).requires_grad_()#.to(device=to_device)\n",
    "            \n",
    "        # x dim add one dummy batch size (1 * seq_len * embedding dim)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(x, (h_0, c_0))\n",
    "        output = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # max on seq length dimension, which is 1. (0 is batch, 2 is embedding)\n",
    "        output = torch.max(output[0], dim=1)[0]  # after max, (max tensor, max_indices)\n",
    "        \n",
    "        output_rank = self.projection(output)\n",
    "        output_cls = self.classifier(output)\n",
    "        \n",
    "        return output_rank, output_cls\n",
    "\n",
    "    def forward(self, x, batch_size=None):\n",
    "        current_bsz = len(x)\n",
    "        list_for_rank = []\n",
    "        list_for_class = []\n",
    "        # x shape: batch * ranklists\n",
    "        # [[1,2],\n",
    "        # [3,4],\n",
    "        # [5,6],\n",
    "        # ...\n",
    "        # [99,100]]\n",
    "        \n",
    "        for i in range(len(x[0])): # 2 dim -> # [[1,3,5],[2,4,6]]\n",
    "            # one column is one batch, feed in one batch of \n",
    "            rank_output, cls_output = self.forward_one(get_column(x,i), batch_size = current_bsz)  \n",
    "            list_for_rank.append(rank_output) # one rank output\n",
    "            list_for_class.append(cls_output) # one column classification output = batch size * num_class\n",
    "        # 2 dim -> # [[1,3,5],[2,4,6]]    \n",
    "        championship = torch.cat(list_for_rank, dim = 1) # [[1,2],[3,4],[5,6]]    \n",
    "        \n",
    "        final_rank_out = self.ranker(championship)\n",
    "        final_cls_out = torch.stack(list_for_class) # [[1,3,5],[2,4,6]]\n",
    "        final_cls_out = final_cls_out.permute(1, 0, 2)\n",
    "        final_cls_out = final_cls_out.reshape(-1,self.class_num) # [num_samples * num_class]\n",
    "\n",
    "        return final_rank_out, final_cls_out\n",
    "\n",
    "    def rank_loss_function(self, y_pred, y_true):        \n",
    "        \n",
    "        return F.cross_entropy(y_pred, y_true)\n",
    "    \n",
    "    def cls_loss_function(self, prediction, target):\n",
    "        return F.cross_entropy(prediction, target)    \n",
    "    \n",
    "    def loss_function(self, rank_loss, cls_loss):\n",
    "        return rank_loss + cls_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text, target = batch\n",
    "        flat_target = torch.flatten(target).long() # for classification, num_sample *1\n",
    "        pairwise_target = label_to_pair_compare(target)#.to(target.device)\n",
    "        \n",
    "        rank_out, cls_out = self(text)\n",
    "\n",
    "        rank_loss = self.rank_loss_function(rank_out, pairwise_target)\n",
    "        cls_loss = self.cls_loss_function(cls_out, flat_target)\n",
    "        \n",
    "        loss = self.loss_function(rank_loss, cls_loss)\n",
    "        \n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text, target = batch\n",
    "        flat_target = torch.flatten(target).long() # for classification, num_sample *1\n",
    "        pairwise_target = label_to_pair_compare(target)#.to(target.device)\n",
    "        rank_out, cls_out = self(text)\n",
    "        \n",
    "        rank_loss = self.rank_loss_function(rank_out, pairwise_target)\n",
    "        cls_loss = self.cls_loss_function(cls_out, flat_target)\n",
    "        \n",
    "        prediction_digits = [torch.argmax(x).item() for x in cls_out]\n",
    "        \n",
    "        val_loss = self.loss_function(rank_loss, cls_loss)\n",
    "\n",
    "        return {'prediction_digits': prediction_digits, 'target': flat_target.tolist(), 'val_loss': val_loss}\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        text, target = batch\n",
    "        flat_target = torch.flatten(target).long() # for classification, num_sample *1\n",
    "        pairwise_target = label_to_pair_compare(target)#.to(target.device)\n",
    "        rank_out, cls_out = self(text)\n",
    "\n",
    "        rank_loss = self.rank_loss_function(rank_out, pairwise_target)\n",
    "        cls_loss = self.cls_loss_function(cls_out, flat_target)\n",
    "        \n",
    "        prediction_digits = [torch.argmax(x).item() for x in cls_out]\n",
    "        \n",
    "        test_loss = self.loss_function(rank_loss, cls_loss)\n",
    "\n",
    "        return {'prediction_digits': prediction_digits, 'target': flat_target.tolist(), 'test_loss': test_loss}\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_val_loss = torch.tensor([x['val_loss'] for x in val_step_outputs]).mean()\n",
    "        val_predictions = [x['prediction_digits'] for x in val_step_outputs]\n",
    "        val_targets = [x['target'] for x in val_step_outputs]\n",
    "        # unpack list of lists\n",
    "        val_predictions = [item for sublist in val_predictions for item in sublist]\n",
    "        val_targets = [item for sublist in val_targets for item in sublist]\n",
    "\n",
    "        cls_report = classification_report(val_targets, val_predictions, digits=4)\n",
    "        print(cls_report)\n",
    "\n",
    "        val_f1 = f1_score(val_targets, val_predictions, average='macro')\n",
    "\n",
    "        return {'avg_val_loss': avg_val_loss, 'val_f1': val_f1}\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_test_loss = torch.tensor([x['test_loss'] for x in test_step_outputs]).mean()\n",
    "        test_predictions = [x['prediction_digits'] for x in test_step_outputs]\n",
    "        test_targets = [x['target'] for x in test_step_outputs]\n",
    "        # unpack list of lists\n",
    "        test_predictions = [item for sublist in test_predictions for item in sublist]\n",
    "        test_targets = [item for sublist in test_targets for item in sublist]\n",
    "\n",
    "        cls_report = classification_report(test_targets, test_predictions, digits=4)\n",
    "        print(cls_report)\n",
    "\n",
    "        test_f1 = f1_score(test_targets, test_predictions, average='macro')\n",
    "\n",
    "        return {'avg_test_loss': avg_test_loss, 'test_f1': test_f1}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=args.lr)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_raw_data = pd.read_pickle(train_file)\n",
    "        train_dataset = MovieScriptDataset(train_raw_data)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=my_collate_fn,\n",
    "            num_workers=10,\n",
    "            drop_last=True)\n",
    "\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dev_raw_data = pd.read_pickle(dev_file)\n",
    "        dev_dataset = MovieScriptDataset(dev_raw_data)\n",
    "        dev_loader = torch.utils.data.DataLoader(\n",
    "            dev_dataset,\n",
    "            batch_size=args.dev_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=my_collate_fn,\n",
    "            drop_last=True)\n",
    "\n",
    "        return dev_loader    \n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_raw_data = pd.read_pickle(test_file)\n",
    "        test_dataset = MovieScriptDataset(test_raw_data)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=args.test_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=my_collate_fn,\n",
    "            drop_last=True)\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieScriptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tabular):\n",
    "        if isinstance(tabular, str):\n",
    "            self.annotations = pd.read_csv(tabular, sep='\\t')\n",
    "        else:\n",
    "            self.annotations = tabular\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(\"one\")\n",
    "        # print(self.annotations.iloc[index, -1])\n",
    "        # print(\"two\")\n",
    "        # print(self.annotations.iloc[index, -2])\n",
    "        text = self.annotations.iloc[index, -1]  # -1 is sent emb index\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, -2]))  # -3 is label index\n",
    "        return {\n",
    "            'text': text,\n",
    "            'label': y_label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "base_dir = 'C:/Users/Jakob/Documents/DSTA_Project/data-science-text-analytics/data/pickle/emb_files/'\n",
    "model_save_dir =  ''\n",
    "\n",
    "train_batch_size = 80\n",
    "dev_batch_size = 2\n",
    "test_batch_size = 2\n",
    "\n",
    "slate_num = 5\n",
    "\n",
    "\n",
    "parser.add_argument('--dev_run', action='store_true')\n",
    "parser.add_argument('--working_aspect_idx', type=int, default=0, help='Apect index as in [frightening, alcohol, nudity, violence, profanity].')\n",
    "parser.add_argument('--base_dir', type=str,default='C:/Users/Jakob/Documents/DSTA_Project/data-science-text-analytics/data/pickle/emb_files/')\n",
    "parser.add_argument('--model_save_dir', type=str, default='./RNN-Trans_S-MT_save/')\n",
    "parser.add_argument('--use_gpu_idx', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=80, help='train_batch_size.')\n",
    "parser.add_argument('--dev_batch_size', type=int, default=2, help='dev_batch_size.')\n",
    "parser.add_argument('--test_batch_size', type=int, default=2, help='test_batch_size.')\n",
    "\n",
    "parser.add_argument('--slate_num', type=int, default=2, help='compare num.')\n",
    "parser.add_argument('--rank_output_size', type=int, default=3, help='rank output num.')\n",
    "parser.add_argument('--cls_output_size', type=int, default=4, help='class num.')\n",
    "parser.add_argument('--input_size', type=int, default=768, help='input dimension.')\n",
    "parser.add_argument('--hidden_size', type=int, default=200, help='RNN hidden dimension.')\n",
    "parser.add_argument('--projection_size', type=int, default=100, help='projection_size dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate.')\n",
    "parser.add_argument('--training_epochs', type=int, default=200, help='Training epochs.')\n",
    "parser.add_argument('--patience', type=int, default=30, help='Early stop patience.')\n",
    "parser.add_argument('--multiple_runs', type=int, default=1, help='Multiple runs of experiment.')\n",
    "parser.add_argument('--numpy_seed', type=int, default=42, help='NumPy seed.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "doc_list = ['frightening', 'alcohol','nudity', 'violence', 'profanity']\n",
    "working_aspect = doc_list[args.working_aspect_idx]\n",
    "\n",
    "print('Now working on:', working_aspect)\n",
    "\n",
    "train_file = args.base_dir + working_aspect + '_train_emb.pkl'\n",
    "dev_file = args.base_dir + working_aspect + '_dev_emb.pkl'\n",
    "test_file = args.base_dir + working_aspect + '_test_emb.pkl'\n",
    "\n",
    "# train_data = pd.read_pickle(train_file)\n",
    "# dev_data = pd.read_pickle(dev_file)\n",
    "# test_data = pd.read_pickle(test_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    for counting in range(args.multiple_runs):\n",
    "        \n",
    "        model = LSTM_model(\n",
    "            args.input_size,\n",
    "            args.slate_num,\n",
    "            args.rank_output_size,\n",
    "            args.cls_output_size,\n",
    "            args.hidden_size,\n",
    "            args.projection_size\n",
    "        )\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor='val_f1',\n",
    "            min_delta=0.00,\n",
    "            patience=args.patience,\n",
    "            verbose=False,\n",
    "            mode='max'\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_f1',\n",
    "            filepath=args.model_save_dir + working_aspect,\n",
    "            mode='max'\n",
    "        )\n",
    " \n",
    "        trainer = pl.Trainer(\n",
    "            fast_dev_run=args.dev_run,\n",
    "            max_epochs=args.training_epochs,\n",
    "            #gpus=[args.use_gpu_idx],\n",
    "            callbacks=[early_stop_callback],\n",
    "            checkpoint_callback=checkpoint_callback\n",
    "        )       \n",
    "        trainer.fit(model)\n",
    "\n",
    "        result = trainer.test()\n",
    "        print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
