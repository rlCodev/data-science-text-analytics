{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import TransformerMixin\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_script_from_id(id):\n",
    "    script = open('../data/script/' + id + '.script', 'r').read()\n",
    "    # print(script)\n",
    "    script = script.replace(\"'\", \" \").replace('\"', ' ').replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').replace('\\b', ' ').replace('\\\\', ' ')\n",
    "    return script\n",
    "\n",
    "def get_pd_dataframe():\n",
    "    inputFile = open('../data_gathering/baseline/output/imdb_id_with_age_rating_and_labels.txt')\n",
    "    df_data = []\n",
    "    for line in inputFile:\n",
    "        line_data = line.strip().split(',')\n",
    "        # print(line_data)\n",
    "        line_data.append(int(line_data[3]) + int(line_data[4]) + int(line_data[5]) + int(line_data[6]))\n",
    "        \n",
    "        max_index = 0\n",
    "        max_value = 0\n",
    "        for i in range(3,7):\n",
    "            vote_count = int(line_data[i])\n",
    "            if(vote_count >= max_value):\n",
    "                max_index = i - 3\n",
    "                max_value = vote_count\n",
    "        line_data.append(max_index)\n",
    "        try:\n",
    "            script = get_script_from_id(line_data[0])\n",
    "        except:\n",
    "            # print('Error on loading script for id: ' + line_data[0])\n",
    "            continue\n",
    "        line_data.append(script)\n",
    "        df_data.append(line_data)\n",
    "\n",
    "    # id | Aspect | None | Mild | Moderate | Severe | Total_votes | Aspect_rating | text\n",
    "    df = pd.DataFrame(df_data, columns=['imdb_id', 'age_rating', 'aspect', 'votes none', 'votes mild', 'votes moderate', 'votes severe', 'total_votes', 'aspect_rating', 'text'])\n",
    "    df.drop(columns=[\"age_rating\"], inplace=True)\n",
    "    df = df.astype({'votes mild':'int', 'votes moderate':'int', 'votes severe':'int', 'votes none':'int', 'total_votes':'int', 'aspect_rating':'int'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leonremke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/leonremke/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leonremke/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# Define a custom transformer to preprocess the text column\n",
    "class TextPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_processed = []\n",
    "        for text in X:\n",
    "            # Convert to lowercase and remove punctuation\n",
    "            text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            # Tokenize the text\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            # Remove stop words and lemmatize the remaining words\n",
    "            processed_tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "            # Join the processed tokens back into a single string\n",
    "            processed_text = ' '.join(processed_tokens)\n",
    "            X_processed.append(processed_text)\n",
    "        return X_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMMovieClassifier():\n",
    "\n",
    "    def __init__(self, df, model_name, num_features):\n",
    "\n",
    "        self.df = df\n",
    "        # Define the column transformer\n",
    "        self.num_features = num_features\n",
    "        self.num_transformer = Pipeline(steps=[\n",
    "            ('normalize', FunctionTransformer(lambda x: x[num_features].div(x['total_votes'], axis=0), validate=False)),\n",
    "            ('scale', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        self.text_transformer = Pipeline(steps=[\n",
    "            ('preprocess', TextPreprocessor()),\n",
    "            ('vectorize', TfidfVectorizer())\n",
    "        ])\n",
    "\n",
    "        self.preprocessor = ColumnTransformer(transformers=[\n",
    "            ('num', self.num_transformer, num_features),\n",
    "            ('text', self.text_transformer, 'text')\n",
    "        ])\n",
    "\n",
    "        # Define the pipeline\n",
    "        self.pipe = Pipeline(steps=[\n",
    "            ('preprocess', self.preprocessor),\n",
    "            ('clf', SVC(C=1, kernel='linear', gamma='auto', degree=3))\n",
    "        ])\n",
    "\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def split_data(self):\n",
    "        # Split the data into training and testing sets\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.df.drop('aspect_rating', axis=1), self.df['aspect_rating'], test_size=0.2, random_state=42)\n",
    "\n",
    "    def train(self):\n",
    "        # Train the pipeline\n",
    "        self.pipe.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def test(self, X_test = None, y_test = None):\n",
    "        # Test the pipeline\n",
    "        if X_test is None or y_test is None:\n",
    "            X_test = self.X_test\n",
    "            y_test = self.y_test\n",
    "\n",
    "        y_pred = self.pipe.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    def predict_and_save(self, X, save_path = \"./\", save_name = \"prediction.csv\"):\n",
    "        # Predict the labels\n",
    "        y_pred = self.pipe.predict(X)\n",
    "        X['prediction'] = y_pred\n",
    "        X.drop(columns=['text'], inplace=True)\n",
    "        X.head()\n",
    "        X.to_csv(save_path + save_name, index=False)\n",
    "\n",
    "    def save_model(self, model_name):\n",
    "        # Save the pipeline\n",
    "        joblib.dump(self.pipe, model_name)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        # Load the pipeline\n",
    "        self.pipe = joblib.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alcohol 477\n",
      "frightening 475\n",
      "nudity 478\n",
      "profanity 477\n",
      "violence 477\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = get_pd_dataframe()\n",
    "\n",
    "# Drop rows with less than 5 votes\n",
    "rows_to_drop = df.loc[df['total_votes'] < 5].index\n",
    "df = df.drop(rows_to_drop)\n",
    "\n",
    "# group the dataframe by 'aspect' and create a dictionary of dataframes\n",
    "df_dict = {aspect: aspect_df.drop('aspect', axis=1) for aspect, aspect_df in df.groupby('aspect')}\n",
    "\n",
    "aspect_classifier_dict = {}\n",
    "\n",
    "# print the dictionary of dataframes\n",
    "for aspect, aspect_df in df_dict.items():\n",
    "    print(aspect, len(aspect_df))\n",
    "    # Limit the number of rows to 50 for testing\n",
    "    aspect_classifier_dict[aspect] = SVMMovieClassifier(aspect_df, 'movie_svm_' + aspect, ['votes none', 'votes mild', 'votes moderate', 'votes severe', 'total_votes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for aspect: alcohol\n",
      "Training model for aspect: frightening\n",
      "Training model for aspect: nudity\n",
      "Training model for aspect: profanity\n",
      "Training model for aspect: violence\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for aspect, movie_classifier in aspect_classifier_dict.items():\n",
    "    try:\n",
    "        print('Training model for aspect: ' + aspect)\n",
    "        movie_classifier.split_data()\n",
    "        movie_classifier.train()\n",
    "    except Exception as e:\n",
    "        print('Error on training model for aspect: ' + aspect)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model for aspect: alcohol\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00        55\n",
      "           2       1.00      1.00      1.00        23\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        96\n",
      "   macro avg       1.00      1.00      1.00        96\n",
      "weighted avg       1.00      1.00      1.00        96\n",
      "\n",
      "Testing model for aspect: frightening\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00        26\n",
      "           2       1.00      1.00      1.00        32\n",
      "           3       1.00      1.00      1.00        24\n",
      "\n",
      "    accuracy                           1.00        95\n",
      "   macro avg       1.00      1.00      1.00        95\n",
      "weighted avg       1.00      1.00      1.00        95\n",
      "\n",
      "Testing model for aspect: nudity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        28\n",
      "           1       1.00      1.00      1.00        39\n",
      "           2       1.00      1.00      1.00        18\n",
      "           3       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        96\n",
      "   macro avg       1.00      1.00      1.00        96\n",
      "weighted avg       1.00      1.00      1.00        96\n",
      "\n",
      "Testing model for aspect: profanity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      1.00      1.00        33\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        96\n",
      "   macro avg       1.00      1.00      1.00        96\n",
      "weighted avg       1.00      1.00      1.00        96\n",
      "\n",
      "Testing model for aspect: violence\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00        28\n",
      "           2       1.00      1.00      1.00        37\n",
      "           3       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        96\n",
      "   macro avg       1.00      1.00      1.00        96\n",
      "weighted avg       1.00      1.00      1.00        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "for aspect, movie_classifier in aspect_classifier_dict.items():\n",
    "    try:\n",
    "        print('Testing model for aspect: ' + aspect)\n",
    "        movie_classifier.test()\n",
    "    except Exception as e:\n",
    "        print('Error on testing model for aspect: ' + aspect + ' Error: ' + str(e))\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting model for aspect: alcohol\n",
      "Predicting model for aspect: frightening\n",
      "Predicting model for aspect: nudity\n",
      "Predicting model for aspect: profanity\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPredicting model for aspect: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m aspect)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     movie_classifier\u001b[39m.\u001b[39;49mpredict_and_save(df_dict[aspect], save_path \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m../data/results_svm/\u001b[39;49m\u001b[39m\"\u001b[39;49m, save_name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mprediction_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m aspect \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mError on testing model for aspect: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m aspect \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m Error: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb Cell 8\u001b[0m in \u001b[0;36mSVMMovieClassifier.predict_and_save\u001b[0;34m(self, X, save_path, save_name)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_and_save\u001b[39m(\u001b[39mself\u001b[39m, X, save_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m\"\u001b[39m, save_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprediction.csv\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# Predict the labels\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpipe\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     X[\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m y_pred\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     X\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/sklearn/utils/metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m attr_err\n\u001b[1;32m    112\u001b[0m     \u001b[39m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/sklearn/pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    467\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[1;32m    468\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 469\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[1;32m    470\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:748\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    744\u001b[0m     \u001b[39m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[1;32m    745\u001b[0m     \u001b[39m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[1;32m    746\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 748\u001b[0m Xs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(\n\u001b[1;32m    749\u001b[0m     X,\n\u001b[1;32m    750\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    751\u001b[0m     _transform_one,\n\u001b[1;32m    752\u001b[0m     fitted\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    753\u001b[0m     column_as_strings\u001b[39m=\u001b[39;49mfit_dataframe_and_transform_dataframe,\n\u001b[1;32m    754\u001b[0m )\n\u001b[1;32m    755\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_output(Xs)\n\u001b[1;32m    757\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m Xs:\n\u001b[1;32m    758\u001b[0m     \u001b[39m# All transformers are None\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:606\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    600\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m    601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[1;32m    602\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    603\u001b[0m     )\n\u001b[1;32m    604\u001b[0m )\n\u001b[1;32m    605\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    607\u001b[0m         delayed(func)(\n\u001b[1;32m    608\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[1;32m    609\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    610\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    611\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m    612\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    613\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    616\u001b[0m     )\n\u001b[1;32m    617\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/sklearn/pipeline.py:876\u001b[0m, in \u001b[0;36m_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform_one\u001b[39m(transformer, X, y, weight, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[0;32m--> 876\u001b[0m     res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m    877\u001b[0m     \u001b[39m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[1;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/sklearn/utils/metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m attr_err\n\u001b[1;32m    112\u001b[0m     \u001b[39m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/sklearn/pipeline.py:647\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    645\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[1;32m    646\u001b[0m \u001b[39mfor\u001b[39;00m _, _, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter():\n\u001b[0;32m--> 647\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[1;32m    648\u001b[0m \u001b[39mreturn\u001b[39;00m Xt\n",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb Cell 8\u001b[0m in \u001b[0;36mTextPreprocessor.transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mtranslate(\u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, string\u001b[39m.\u001b[39mpunctuation))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Tokenize the text\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Remove stop words and lemmatize the remaining words\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/dsta-movie-analytics/own_model/svm_pipeline_user_reports.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m processed_tokens \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlemmatizer\u001b[39m.\u001b[39mlemmatize(token) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/nltk/tokenize/destructive.py:178\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    175\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m text \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[39mfor\u001b[39;00m regexp, substitution \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mENDING_QUOTES:\n\u001b[0;32m--> 178\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39;49msub(substitution, text)\n\u001b[1;32m    180\u001b[0m \u001b[39mfor\u001b[39;00m regexp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m    181\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1 \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2 \u001b[39m\u001b[39m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for aspect, movie_classifier in aspect_classifier_dict.items():\n",
    "    try:\n",
    "        print('Predicting model for aspect: ' + aspect)\n",
    "        movie_classifier.predict_and_save(df_dict[aspect], save_path = \"../data/results_svm/\", save_name = \"prediction_\" + aspect + \".csv\")\n",
    "    except Exception as e:\n",
    "        print('Error on predicting model for aspect: ' + aspect + ' Error: ' + str(e))\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSTA-Ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
